"""
Simple Evaluator for Chatbot

ÄÃ¡nh giÃ¡ chatbot Ä‘Æ¡n giáº£n vá»›i 3 loáº¡i cÃ¢u há»i:
1. TRUE/FALSE
2. YES/NO
3. MCQ
"""

import json
import logging
import time
from typing import Dict, List, Tuple, Optional
from pathlib import Path
from datetime import datetime

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class SimpleEvaluator:
    """ÄÃ¡nh giÃ¡ chatbot trÃªn táº­p dá»¯ liá»‡u."""
    
    def __init__(self, dataset_path: str):
        """
        Args:
            dataset_path: ÄÆ°á»ng dáº«n Ä‘áº¿n file JSON chá»©a cÃ¢u há»i
        """
        with open(dataset_path, 'r', encoding='utf-8') as f:
            self.questions = json.load(f)
        logger.info(f"Loaded {len(self.questions)} questions")
    
    def evaluate_chatbot(self, chatbot, max_questions: int = None) -> Dict:
        """
        ÄÃ¡nh giÃ¡ chatbot.
        
        Args:
            chatbot: Chatbot cÃ³ cÃ¡c method:
                - answer_true_false(question, statement) -> (answer, confidence)
                - answer_yes_no(question) -> (answer, confidence)
                - answer_mcq(question, choices) -> (answer, confidence)
            max_questions: Sá»‘ cÃ¢u há»i tá»‘i Ä‘a Ä‘á»ƒ Ä‘Ã¡nh giÃ¡
            
        Returns:
            Dict chá»©a káº¿t quáº£ Ä‘Ã¡nh giÃ¡
        """
        questions = self.questions[:max_questions] if max_questions else self.questions
        
        results = {
            "total": len(questions),
            "correct": 0,
            "wrong": 0,
            "by_type": {
                "true_false": {"total": 0, "correct": 0},
                "yes_no": {"total": 0, "correct": 0},
                "mcq": {"total": 0, "correct": 0}
            },
            "by_category": {},
            "by_hops": {
                "1-hop": {"total": 0, "correct": 0},
                "2-hop": {"total": 0, "correct": 0}
            },
            "errors": []
        }
        
        start_time = time.time()
        
        for i, q in enumerate(questions):
            if (i + 1) % 100 == 0:
                logger.info(f"Progress: {i+1}/{len(questions)}")
            
            try:
                predicted = None
                
                if q["type"] == "true_false":
                    pred, conf = chatbot.answer_true_false(q["question"], q["statement"])
                    predicted = pred
                    
                elif q["type"] == "yes_no":
                    pred, conf = chatbot.answer_yes_no(q["question"])
                    predicted = pred
                    
                elif q["type"] == "mcq":
                    pred, conf = chatbot.answer_mcq(q["question"], q["choices"])
                    predicted = pred
                
                # Check correctness
                is_correct = (predicted == q["answer"])
                
                # Update stats
                results["by_type"][q["type"]]["total"] += 1
                
                if is_correct:
                    results["correct"] += 1
                    results["by_type"][q["type"]]["correct"] += 1
                else:
                    results["wrong"] += 1
                    results["errors"].append({
                        "id": q["id"],
                        "type": q["type"],
                        "question": q["question"],
                        "expected": q["answer"],
                        "predicted": predicted,
                        "category": q.get("category")
                    })
                
                # By category
                cat = q.get("category", "unknown")
                if cat not in results["by_category"]:
                    results["by_category"][cat] = {"total": 0, "correct": 0}
                results["by_category"][cat]["total"] += 1
                if is_correct:
                    results["by_category"][cat]["correct"] += 1
                
                # By hops
                hops = q.get("hops", 1)
                hop_key = f"{hops}-hop"
                results["by_hops"][hop_key]["total"] += 1
                if is_correct:
                    results["by_hops"][hop_key]["correct"] += 1
                    
            except Exception as e:
                logger.error(f"Error on question {q['id']}: {e}")
                results["wrong"] += 1
                results["errors"].append({
                    "id": q["id"],
                    "error": str(e)
                })
        
        elapsed = time.time() - start_time
        
        # Calculate percentages
        results["accuracy"] = results["correct"] / results["total"] * 100 if results["total"] > 0 else 0
        
        for qtype in results["by_type"]:
            total = results["by_type"][qtype]["total"]
            correct = results["by_type"][qtype]["correct"]
            results["by_type"][qtype]["accuracy"] = correct / total * 100 if total > 0 else 0
        
        for cat in results["by_category"]:
            total = results["by_category"][cat]["total"]
            correct = results["by_category"][cat]["correct"]
            results["by_category"][cat]["accuracy"] = correct / total * 100 if total > 0 else 0
        
        for hop in results["by_hops"]:
            total = results["by_hops"][hop]["total"]
            correct = results["by_hops"][hop]["correct"]
            results["by_hops"][hop]["accuracy"] = correct / total * 100 if total > 0 else 0
        
        results["elapsed_seconds"] = elapsed
        results["timestamp"] = datetime.now().isoformat()
        
        return results
    
    def print_results(self, results: Dict):
        """In káº¿t quáº£ Ä‘Ã¡nh giÃ¡."""
        print("\n" + "=" * 60)
        print("ðŸ“Š Káº¾T QUáº¢ ÄÃNH GIÃ CHATBOT")
        print("=" * 60)
        
        print(f"\nâœ… Tá»•ng sá»‘ cÃ¢u há»i: {results['total']}")
        print(f"âœ… Tráº£ lá»i Ä‘Ãºng: {results['correct']}")
        print(f"âŒ Tráº£ lá»i sai: {results['wrong']}")
        print(f"ðŸ“ˆ Äá»™ chÃ­nh xÃ¡c: {results['accuracy']:.2f}%")
        
        print("\n--- Theo loáº¡i cÃ¢u há»i ---")
        for qtype, stats in results["by_type"].items():
            if stats["total"] > 0:
                print(f"  {qtype}: {stats['correct']}/{stats['total']} ({stats['accuracy']:.2f}%)")
        
        print("\n--- Theo sá»‘ hop ---")
        for hop, stats in results["by_hops"].items():
            if stats["total"] > 0:
                print(f"  {hop}: {stats['correct']}/{stats['total']} ({stats['accuracy']:.2f}%)")
        
        print("\n--- Theo danh má»¥c ---")
        for cat, stats in sorted(results["by_category"].items()):
            if stats["total"] > 0:
                print(f"  {cat}: {stats['correct']}/{stats['total']} ({stats['accuracy']:.2f}%)")
        
        print(f"\nâ±ï¸ Thá»i gian: {results['elapsed_seconds']:.2f}s")
        
        if results["errors"][:5]:
            print("\n--- Má»™t sá»‘ lá»—i máº«u ---")
            for err in results["errors"][:5]:
                print(f"  Q: {err.get('question', 'N/A')[:80]}...")
                print(f"     Expected: {err.get('expected')} | Got: {err.get('predicted')}")
                print()
    
    def save_results(self, results: Dict, filepath: str):
        """LÆ°u káº¿t quáº£ ra file."""
        Path(filepath).parent.mkdir(parents=True, exist_ok=True)
        
        # Only save first 100 errors to keep file size manageable
        results_to_save = results.copy()
        results_to_save["errors"] = results["errors"][:100]
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results_to_save, f, ensure_ascii=False, indent=2)
        
        logger.info(f"Saved results to {filepath}")


def evaluate_simple_chatbot():
    """ÄÃ¡nh giÃ¡ SimpleKGChatbot."""
    from chatbot.simple_chatbot import SimpleKGChatbot
    
    chatbot = SimpleKGChatbot()
    if not chatbot.initialize():
        print("Failed to initialize chatbot")
        return
    
    evaluator = SimpleEvaluator("data/evaluation/simple_eval_dataset.json")
    results = evaluator.evaluate_chatbot(chatbot)
    evaluator.print_results(results)
    evaluator.save_results(results, "reports/simple_chatbot_eval.json")


def evaluate_with_gemini():
    """ÄÃ¡nh giÃ¡ vá»›i Gemini API."""
    import google.generativeai as genai
    import os
    
    api_key = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
    if not api_key:
        print("GEMINI_API_KEY not set")
        return
    
    genai.configure(api_key=api_key)
    model = genai.GenerativeModel('gemini-2.0-flash-exp')
    
    class GeminiWrapper:
        def __init__(self, model):
            self.model = model
        
        def answer_true_false(self, question: str, statement: str) -> Tuple[str, float]:
            prompt = f"""Báº¡n lÃ  chuyÃªn gia vá» bÃ³ng Ä‘Ã¡ Viá»‡t Nam.
HÃ£y tráº£ lá»i cÃ¢u há»i sau báº±ng TRUE hoáº·c FALSE.
Chá»‰ tráº£ lá»i TRUE hoáº·c FALSE, khÃ´ng giáº£i thÃ­ch.

CÃ¢u há»i: {question}
Má»‡nh Ä‘á»: {statement}

Tráº£ lá»i:"""
            try:
                response = self.model.generate_content(prompt)
                answer = response.text.strip().upper()
                if "TRUE" in answer:
                    return "TRUE", 1.0
                return "FALSE", 1.0
            except Exception as e:
                logger.error(f"Gemini error: {e}")
                return "FALSE", 0.5
        
        def answer_yes_no(self, question: str) -> Tuple[str, float]:
            prompt = f"""Báº¡n lÃ  chuyÃªn gia vá» bÃ³ng Ä‘Ã¡ Viá»‡t Nam.
HÃ£y tráº£ lá»i cÃ¢u há»i sau báº±ng YES hoáº·c NO.
Chá»‰ tráº£ lá»i YES hoáº·c NO, khÃ´ng giáº£i thÃ­ch.

CÃ¢u há»i: {question}

Tráº£ lá»i:"""
            try:
                response = self.model.generate_content(prompt)
                answer = response.text.strip().upper()
                if "YES" in answer or "CÃ“" in answer.upper():
                    return "YES", 1.0
                return "NO", 1.0
            except Exception as e:
                logger.error(f"Gemini error: {e}")
                return "NO", 0.5
        
        def answer_mcq(self, question: str, choices: List[str]) -> Tuple[str, float]:
            choices_text = "\n".join([f"{chr(65+i)}. {c}" for i, c in enumerate(choices)])
            prompt = f"""Báº¡n lÃ  chuyÃªn gia vá» bÃ³ng Ä‘Ã¡ Viá»‡t Nam.
HÃ£y chá»n Ä‘Ã¡p Ã¡n Ä‘Ãºng cho cÃ¢u há»i tráº¯c nghiá»‡m sau.
Chá»‰ tráº£ lá»i báº±ng ná»™i dung Ä‘Ã¡p Ã¡n (khÃ´ng cáº§n A, B, C, D).

CÃ¢u há»i: {question}
{choices_text}

ÄÃ¡p Ã¡n Ä‘Ãºng lÃ :"""
            try:
                response = self.model.generate_content(prompt)
                answer = response.text.strip()
                
                # TÃ¬m Ä‘Ã¡p Ã¡n khá»›p nháº¥t
                for choice in choices:
                    if choice.lower() in answer.lower() or answer.lower() in choice.lower():
                        return choice, 1.0
                
                # Kiá»ƒm tra theo chá»¯ cÃ¡i
                for i, choice in enumerate(choices):
                    if chr(65+i) in answer.upper():
                        return choice, 1.0
                
                return choices[0], 0.3
            except Exception as e:
                logger.error(f"Gemini error: {e}")
                return choices[0], 0.3
    
    gemini = GeminiWrapper(model)
    
    evaluator = SimpleEvaluator("data/evaluation/simple_eval_dataset.json")
    # Chá»‰ test 500 cÃ¢u Ä‘á»ƒ trÃ¡nh rate limit
    results = evaluator.evaluate_chatbot(gemini, max_questions=500)
    evaluator.print_results(results)
    evaluator.save_results(results, "reports/gemini_eval.json")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "--gemini":
        evaluate_with_gemini()
    else:
        evaluate_simple_chatbot()
