"""
Vietnam Football Knowledge Graph Chatbot

Main chatbot implementation with:
- Small Language Model (Qwen2-0.5B or similar < 1B params)
- GraphRAG for knowledge retrieval
- Multi-hop reasoning
"""

import logging
import torch
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import re

from .config import (
    MODEL_NAME, MODEL_MAX_LENGTH, MODEL_TEMPERATURE, MODEL_TOP_P,
    DEVICE, TORCH_DTYPE, MAX_GRAPH_CONTEXT_LENGTH
)
from .knowledge_graph import KnowledgeGraph, Entity, Relationship, get_kg
from .multi_hop_reasoning import MultiHopReasoner, ReasoningChain, QueryType

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class ChatResponse:
    """Response from the chatbot."""
    answer: str
    confidence: float
    reasoning_chain: Optional[ReasoningChain]
    sources: List[str]
    model_used: str


class GraphRAGChatbot:
    """
    Knowledge Graph-based Chatbot with RAG and Multi-hop Reasoning.
    
    Architecture:
    1. Entity Recognition - Identify entities in user query
    2. Graph Retrieval - Retrieve relevant subgraph from Neo4j
    3. Multi-hop Reasoning - Traverse graph for complex queries
    4. LLM Generation - Generate natural language response
    """
    
    def __init__(self, model_name: str = None, device: str = None):
        self.model_name = model_name or MODEL_NAME
        self.device = device or DEVICE
        
        self.model = None
        self.tokenizer = None
        self.generator = None
        
        self.kg = None
        self.reasoner = None
        
        self._initialized = False
        
    def initialize(self) -> bool:
        """Initialize all components."""
        logger.info("üöÄ Initializing GraphRAG Chatbot...")
        
        # Initialize Knowledge Graph
        logger.info("üìä Connecting to Knowledge Graph...")
        self.kg = KnowledgeGraph()
        if not self.kg.connect():
            logger.error("Failed to connect to Knowledge Graph")
            return False
            
        # Initialize Multi-hop Reasoner
        self.reasoner = MultiHopReasoner(self.kg)
        logger.info("üß† Multi-hop Reasoner initialized")
        
        # Initialize Language Model
        logger.info(f"ü§ñ Loading Language Model: {self.model_name}")
        if not self._load_model():
            logger.warning("Failed to load LLM, will use rule-based responses")
            
        self._initialized = True
        logger.info("‚úÖ Chatbot initialized successfully!")
        return True
        
    def _load_model(self) -> bool:
        """Load the small language model."""
        try:
            # Determine dtype
            if self.device == "cuda" and torch.cuda.is_available():
                dtype = torch.float16
                device_map = "auto"
            else:
                dtype = torch.float32
                device_map = None
                self.device = "cpu"
                
            logger.info(f"Loading model on {self.device}...")
            
            # Load tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True
            )
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                
            # Load model with efficient settings
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=dtype,
                device_map=device_map,
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            
            # Create text generation pipeline
            self.generator = pipeline(
                "text-generation",
                model=self.model,
                tokenizer=self.tokenizer,
                device_map=device_map,
                torch_dtype=dtype
            )
            
            logger.info(f"‚úÖ Model loaded: {self.model_name}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Failed to load model: {e}")
            return False
            
    def chat(self, user_input: str, use_reasoning: bool = True) -> ChatResponse:
        """
        Process user input and generate response.
        
        Args:
            user_input: User's question or message
            use_reasoning: Whether to use multi-hop reasoning
            
        Returns:
            ChatResponse with answer and metadata
        """
        if not self._initialized:
            if not self.initialize():
                return ChatResponse(
                    answer="L·ªói: Chatbot ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o.",
                    confidence=0.0,
                    reasoning_chain=None,
                    sources=[],
                    model_used="none"
                )
                
        # Step 1: Extract entities from user input
        entities = self._extract_entities(user_input)
        logger.info(f"Extracted entities: {entities}")
        
        # Step 2: Perform multi-hop reasoning
        reasoning_chain = None
        if use_reasoning and entities:
            reasoning_chain = self.reasoner.reason(user_input, entities)
            logger.info(f"Reasoning type: {reasoning_chain.query_type.value}")
            
        # Step 3: Build context from graph
        context = self._build_graph_context(user_input, entities, reasoning_chain)
        
        # Step 4: Generate response
        if self.generator:
            answer = self._generate_with_llm(user_input, context, reasoning_chain)
        else:
            answer = self._generate_rule_based(user_input, context, reasoning_chain)
            
        # Build response
        sources = []
        if reasoning_chain:
            sources = reasoning_chain.evidence[:5]
            
        return ChatResponse(
            answer=answer,
            confidence=reasoning_chain.confidence if reasoning_chain else 0.5,
            reasoning_chain=reasoning_chain,
            sources=sources,
            model_used=self.model_name if self.generator else "rule-based"
        )
        
    def _extract_entities(self, text: str) -> List[str]:
        """Extract entities from text using graph search."""
        entities = []
        
        # Clean text
        text = re.sub(r'[?!.,;:]', ' ', text)
        words = text.split()
        
        # Try n-grams from longest to shortest
        used_indices = set()
        
        for n in range(min(5, len(words)), 0, -1):
            for i in range(len(words) - n + 1):
                # Skip if any word already used
                if any(idx in used_indices for idx in range(i, i + n)):
                    continue
                    
                phrase = " ".join(words[i:i+n])
                
                # Skip common Vietnamese words
                skip_words = {
                    'l√†', 'c·ªßa', 'c√≥', 'v√†', 'hay', 'kh√¥ng', 'ƒë√£', 't·ª´ng',
                    'ƒë∆∞·ª£c', 'nh·ªØng', 'c√°c', 'm·ªôt', 'cho', 'v·ªõi', 'trong',
                    'n√†y', 'ƒë√≥', 'nh∆∞', 'th·∫ø', 'n√†o', 'g√¨', 'ai', 'bao',
                    'nhi√™u', 'sao', 'v√¨', 'khi', 'n·∫øu', 'nh∆∞ng', 'm√†',
                    'c·∫ßu', 'th·ªß', 'ƒë·ªôi', 'b√≥ng', 'c√¢u', 'l·∫°c', 'b·ªô'
                }
                
                if phrase.lower() in skip_words:
                    continue
                    
                # Search in knowledge graph
                matches = self.kg.search_entities(phrase, limit=1)
                if matches:
                    entities.append(matches[0].name)
                    used_indices.update(range(i, i + n))
                    
        return list(dict.fromkeys(entities))  # Remove duplicates, preserve order
        
    def _build_graph_context(self, question: str, entities: List[str],
                             reasoning_chain: Optional[ReasoningChain]) -> str:
        """Build context string from graph information."""
        context_parts = []
        
        # Add reasoning evidence
        if reasoning_chain and reasoning_chain.evidence:
            context_parts.append("Th√¥ng tin t·ª´ ƒë·ªì th·ªã tri th·ª©c:")
            for ev in reasoning_chain.evidence[:10]:
                context_parts.append(f"- {ev}")
                
        # Add entity information
        for entity_name in entities[:3]:
            entity = self.kg.get_entity_by_name(entity_name)
            if entity:
                context_parts.append(f"\n{entity.to_text()}")
                
                # Add key relationships
                rels = self.kg.get_entity_relationships(entity_name)[:5]
                for rel in rels:
                    context_parts.append(f"- {rel.to_text()}")
                    
        context = "\n".join(context_parts)
        
        # Truncate if too long
        if len(context) > MAX_GRAPH_CONTEXT_LENGTH:
            context = context[:MAX_GRAPH_CONTEXT_LENGTH] + "..."
            
        return context
        
    def _generate_with_llm(self, question: str, context: str,
                           reasoning_chain: Optional[ReasoningChain]) -> str:
        """Generate response using the language model."""
        # Build prompt
        prompt = self._build_prompt(question, context, reasoning_chain)
        
        try:
            # Generate response
            outputs = self.generator(
                prompt,
                max_new_tokens=256,
                temperature=MODEL_TEMPERATURE,
                top_p=MODEL_TOP_P,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                return_full_text=False
            )
            
            response = outputs[0]['generated_text'].strip()
            
            # Clean up response
            response = self._clean_response(response)
            
            return response
            
        except Exception as e:
            logger.error(f"LLM generation error: {e}")
            return self._generate_rule_based(question, context, reasoning_chain)
            
    def _build_prompt(self, question: str, context: str,
                      reasoning_chain: Optional[ReasoningChain]) -> str:
        """Build prompt for the language model."""
        
        system_prompt = """B·∫°n l√† tr·ª£ l√Ω AI chuy√™n v·ªÅ b√≥ng ƒë√° Vi·ªát Nam. Tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin t·ª´ ƒë·ªì th·ªã tri th·ª©c ƒë∆∞·ª£c cung c·∫•p.
Quy t·∫Øc:
- Ch·ªâ tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p
- N·∫øu kh√¥ng c√≥ th√¥ng tin, h√£y n√≥i "T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin v·ªÅ ƒëi·ªÅu n√†y"
- Tr·∫£ l·ªùi ng·∫Øn g·ªçn, ch√≠nh x√°c"""

        # Build context section
        context_section = f"\nTh√¥ng tin t·ª´ ƒë·ªì th·ªã tri th·ª©c:\n{context}" if context else ""
        
        # Add reasoning if available
        reasoning_section = ""
        if reasoning_chain and reasoning_chain.final_answer:
            reasoning_section = f"\nK·∫øt qu·∫£ suy lu·∫≠n: {reasoning_chain.final_answer}"
            
        # Format based on model type
        if "qwen" in self.model_name.lower():
            prompt = f"""<|im_start|>system
{system_prompt}<|im_end|>
<|im_start|>user
{context_section}
{reasoning_section}

C√¢u h·ªèi: {question}<|im_end|>
<|im_start|>assistant
"""
        elif "llama" in self.model_name.lower() or "tinyllama" in self.model_name.lower():
            prompt = f"""<|system|>
{system_prompt}</s>
<|user|>
{context_section}
{reasoning_section}

C√¢u h·ªèi: {question}</s>
<|assistant|>
"""
        else:
            prompt = f"""{system_prompt}

{context_section}
{reasoning_section}

C√¢u h·ªèi: {question}

Tr·∫£ l·ªùi:"""
            
        return prompt
        
    def _clean_response(self, response: str) -> str:
        """Clean up the generated response."""
        # Remove special tokens
        response = re.sub(r'<\|.*?\|>', '', response)
        response = re.sub(r'</s>', '', response)
        
        # Remove repetitions
        sentences = response.split('.')
        seen = set()
        unique_sentences = []
        for s in sentences:
            s = s.strip()
            if s and s not in seen:
                seen.add(s)
                unique_sentences.append(s)
                
        response = '. '.join(unique_sentences)
        if response and not response.endswith('.'):
            response += '.'
            
        return response.strip()
        
    def _generate_rule_based(self, question: str, context: str,
                             reasoning_chain: Optional[ReasoningChain]) -> str:
        """Generate response using rules (fallback when no LLM)."""
        if reasoning_chain:
            return reasoning_chain.final_answer
            
        if context:
            return f"D·ª±a tr√™n ƒë·ªì th·ªã tri th·ª©c:\n{context}"
            
        return "T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong ƒë·ªì th·ªã tri th·ª©c."
        
    def answer_yes_no(self, question: str) -> Tuple[str, float, str]:
        """
        Answer a yes/no question.
        
        Returns:
            Tuple of (answer: "ƒê√∫ng"/"Sai", confidence, explanation)
        """
        entities = self._extract_entities(question)
        
        if self.reasoner:
            is_true, confidence, explanation = self.reasoner.answer_yes_no(question, entities)
            answer = "ƒê√∫ng" if is_true else "Sai"
            return answer, confidence, explanation
            
        return "Kh√¥ng r√µ", 0.0, "Kh√¥ng th·ªÉ x√°c ƒë·ªãnh"
        
    def answer_mcq(self, question: str, choices: List[str]) -> Tuple[str, float, str]:
        """
        Answer a multiple choice question.
        
        Returns:
            Tuple of (selected_choice, confidence, explanation)
        """
        entities = self._extract_entities(question)
        
        # Evaluate each choice
        best_choice = None
        best_confidence = 0.0
        best_explanation = ""
        
        for choice in choices:
            # Combine question with choice
            full_question = f"{question} {choice}"
            choice_entities = entities + self._extract_entities(choice)
            
            # Reason about this choice
            chain = self.reasoner.reason(full_question, choice_entities)
            
            if chain.confidence > best_confidence:
                best_confidence = chain.confidence
                best_choice = choice
                best_explanation = chain.final_answer
                
        return best_choice or choices[0], best_confidence, best_explanation
        
    def close(self):
        """Clean up resources."""
        if self.kg:
            self.kg.close()
        if self.model:
            del self.model
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            

class SimpleChatbot:
    """
    Simplified chatbot without LLM - uses only graph reasoning.
    Faster and lighter, good for evaluation.
    """
    
    def __init__(self):
        self.kg = None
        self.reasoner = None
        
    def initialize(self) -> bool:
        """Initialize the chatbot."""
        self.kg = KnowledgeGraph()
        if not self.kg.connect():
            return False
        self.reasoner = MultiHopReasoner(self.kg)
        return True
        
    def chat(self, question: str) -> str:
        """Answer a question using graph reasoning only."""
        question_lower = question.lower()
        
        # Check if this is a question about national team
        if self._is_national_team_question(question_lower):
            return self._answer_national_team_question(question)
        
        # Check if this is a birthplace question
        if self._is_birthplace_question(question_lower):
            return self._answer_birthplace_question(question)
        
        # Check if this is a Yes/No question about playing for a club
        if self._is_played_for_question(question_lower):
            return self._answer_played_for_question(question)
        
        # Check if this is a general Yes/No question
        if self._is_yes_no_question(question_lower):
            answer, confidence = self.answer_yes_no(question)
            return answer
        
        # Default: use general reasoning
        entities = self._extract_entities(question)
        chain = self.reasoner.reason(question, entities)
        return chain.final_answer
    
    def _is_national_team_question(self, question_lower: str) -> bool:
        """Check if question asks about national team."""
        patterns = ['ƒë·ªôi tuy·ªÉn', 'qu·ªëc gia', 'tuy·ªÉn vi·ªát nam', 'tuy·ªÉn qu·ªëc gia', 'kho√°c √°o ƒë·ªôi tuy·ªÉn']
        return any(p in question_lower for p in patterns)
    
    def _is_birthplace_question(self, question_lower: str) -> bool:
        """Check if question asks about birthplace."""
        patterns = ['sinh ra', 'sinh ·ªü', 'qu√™ ·ªü', 'qu√™', 'ƒë·∫øn t·ª´', 'n∆°i sinh', '·ªü ƒë√¢u']
        return any(p in question_lower for p in patterns)
    
    def _answer_national_team_question(self, question: str) -> str:
        """Answer question about national team."""
        player = self._extract_player_from_question(question)
        
        if not player:
            entities = self._extract_entities(question)
            for entity in entities:
                result = self.kg.execute_cypher(
                    "MATCH (p:Player {name: $name}) RETURN p LIMIT 1",
                    {"name": entity}
                )
                if result:
                    player = entity
                    break
        
        if not player:
            return "Kh√¥ng t√¨m th·∫•y c·∫ßu th·ªß trong c√¢u h·ªèi."
        
        # Check national team
        result = self.kg.execute_cypher("""
            MATCH (p:Player {name: $name})-[:PLAYED_FOR_NATIONAL]->(n:NationalTeam)
            RETURN n.name as team
        """, {"name": player})
        
        if result:
            teams = [r['team'] for r in result]
            teams_str = ", ".join(teams)
            return f"C√≥, {player} t·ª´ng kho√°c √°o ƒë·ªôi tuy·ªÉn: {teams_str}."
        else:
            return f"Kh√¥ng, {player} ch∆∞a t·ª´ng kho√°c √°o ƒë·ªôi tuy·ªÉn qu·ªëc gia."
    
    def _answer_birthplace_question(self, question: str) -> str:
        """Answer question about birthplace."""
        player = self._extract_player_from_question(question)
        
        if not player:
            entities = self._extract_entities(question)
            for entity in entities:
                result = self.kg.execute_cypher(
                    "MATCH (p:Player {name: $name}) RETURN p LIMIT 1",
                    {"name": entity}
                )
                if result:
                    player = entity
                    break
        
        if not player:
            return "Kh√¥ng t√¨m th·∫•y c·∫ßu th·ªß trong c√¢u h·ªèi."
        
        # Get birthplace
        result = self.kg.execute_cypher("""
            MATCH (p:Player {name: $name})-[:BORN_IN]->(pr:Province)
            RETURN pr.name as province
        """, {"name": player})
        
        if result:
            province = result[0]['province']
            return f"{player} sinh ra t·∫°i {province}."
        else:
            return f"Kh√¥ng t√¨m th·∫•y th√¥ng tin n∆°i sinh c·ªßa {player}."
    
    def _is_played_for_question(self, question_lower: str) -> bool:
        """Check if question asks about player playing for a club."""
        # NOT a same-club question
        if 'c√πng c√¢u l·∫°c b·ªô' in question_lower or 'ch∆°i c√πng' in question_lower:
            return False
        patterns = [
            'ch∆°i cho', 'thi ƒë·∫•u cho', 'kho√°c √°o', 't·ª´ng ch∆°i', 
            'ƒë√° cho', 'thu·ªôc', '·ªü clb', '·ªü c√¢u l·∫°c b·ªô'
        ]
        return any(p in question_lower for p in patterns)
    
    def _is_yes_no_question(self, question_lower: str) -> bool:
        """Check if this is a Yes/No question."""
        patterns = [
            'c√≥ ph·∫£i', 'c√≥ ph·∫£i l√†', 'ƒë√∫ng kh√¥ng', 'ph·∫£i kh√¥ng',
            'c√≥ t·ª´ng', 'ƒë√£ t·ª´ng', 'kh√¥ng?', 'ch∆∞a?', '√†?'
        ]
        return any(p in question_lower for p in patterns)
    
    def _answer_played_for_question(self, question: str) -> str:
        """Answer a specific question about player playing for club."""
        question_lower = question.lower()
        
        # First, try to identify club from common abbreviations and names
        club = self._extract_club_from_question(question_lower)
        
        # Then find player - prioritize finding player name in question
        player = self._extract_player_from_question(question)
        
        if not player:
            entities = self._extract_entities(question)
            # Find player from entities
            for entity in entities:
                result = self.kg.execute_cypher(
                    "MATCH (p:Player {name: $name}) RETURN p LIMIT 1",
                    {"name": entity}
                )
                if result:
                    player = entity
                    break
        
        if not player:
            return f"Kh√¥ng t√¨m th·∫•y c·∫ßu th·ªß trong c√¢u h·ªèi."
        
        if not club:
            return f"Kh√¥ng t√¨m th·∫•y c√¢u l·∫°c b·ªô trong c√¢u h·ªèi. B·∫°n h·ªèi v·ªÅ CLB n√†o?"
        
        # Check if player played for club
        played = self._check_played_for(player, club)
        
        if played:
            return f"C√≥, {player} t·ª´ng ch∆°i cho {club}."
        else:
            # Get clubs the player actually played for
            clubs = self._get_player_clubs(player)
            if clubs:
                clubs_str = ", ".join(clubs[:5])
                return f"Kh√¥ng, {player} kh√¥ng t·ª´ng ch∆°i cho {club}. {player} t·ª´ng ch∆°i cho: {clubs_str}."
            return f"Kh√¥ng, {player} kh√¥ng t·ª´ng ch∆°i cho {club}."
    
    def _extract_club_from_question(self, question_lower: str) -> Optional[str]:
        """Extract club name from question, handling abbreviations."""
        # Dictionary of common club names and abbreviations
        club_patterns = {
            'c√¥ng an h√† n·ªôi': 'C√¥ng an H√† N·ªôi',
            'cahn': 'C√¥ng an H√† N·ªôi',
            'slna': 'C√¢u l·∫°c b·ªô b√≥ng ƒë√° S√¥ng Lam Ngh·ªá An',
            's√¥ng lam ngh·ªá an': 'C√¢u l·∫°c b·ªô b√≥ng ƒë√° S√¥ng Lam Ngh·ªá An',
            's√¥ng lam': 'C√¢u l·∫°c b·ªô b√≥ng ƒë√° S√¥ng Lam Ngh·ªá An',
            'hagl': 'C√¢u l·∫°c b·ªô b√≥ng ƒë√° Ho√†ng Anh Gia Lai',
            'ho√†ng anh gia lai': 'C√¢u l·∫°c b·ªô b√≥ng ƒë√° Ho√†ng Anh Gia Lai',
            'gia lai': 'C√¢u l·∫°c b·ªô b√≥ng ƒë√° Ho√†ng Anh Gia Lai',
            'h√† n·ªôi fc': 'H√† N·ªôi',
            'clb h√† n·ªôi': 'H√† N·ªôi',
            'viettel': 'C√¢u l·∫°c b·ªô b√≥ng ƒë√° Th·ªÉ C√¥ng ‚Äì Viettel',
            'th·ªÉ c√¥ng': 'C√¢u l·∫°c b·ªô b√≥ng ƒë√° Th·ªÉ C√¥ng ‚Äì Viettel',
            'b√¨nh d∆∞∆°ng': 'C√¢u l·∫°c b·ªô b√≥ng ƒë√° B√¨nh D∆∞∆°ng',
            'ƒë√† n·∫µng': 'C√¢u l·∫°c b·ªô b√≥ng ƒë√° SHB ƒê√† N·∫µng',
            'shb ƒë√† n·∫µng': 'C√¢u l·∫°c b·ªô b√≥ng ƒë√° SHB ƒê√† N·∫µng',
            'h·∫£i ph√≤ng': 'C√¢u l·∫°c b·ªô b√≥ng ƒë√° H·∫£i Ph√≤ng',
            'thanh h√≥a': 'C√¢u l·∫°c b·ªô b√≥ng ƒë√° ƒê√¥ng √Å Thanh H√≥a',
            'nam ƒë·ªãnh': 'C√¢u l·∫°c b·ªô b√≥ng ƒë√° Th√©p Xanh Nam ƒê·ªãnh',
            'qu·∫£ng nam': 'C√¢u l·∫°c b·ªô b√≥ng ƒë√° Qu·∫£ng Nam',
            'tp.hcm': 'C√¢u l·∫°c b·ªô b√≥ng ƒë√° C√¥ng an Th√†nh ph·ªë H·ªì Ch√≠ Minh',
            'tp hcm': 'C√¢u l·∫°c b·ªô b√≥ng ƒë√° C√¥ng an Th√†nh ph·ªë H·ªì Ch√≠ Minh',
        }
        
        # Check for abbreviations first (longer patterns first)
        for pattern, club_name in sorted(club_patterns.items(), key=lambda x: -len(x[0])):
            if pattern in question_lower:
                return club_name
        
        # Special case: "H√† N·ªôi" without "C√¥ng an"
        if 'h√† n·ªôi' in question_lower and 'c√¥ng an' not in question_lower:
            return 'H√† N·ªôi'
        
        return None
    
    def _extract_player_from_question(self, question: str) -> Optional[str]:
        """Extract player name from question, handling common short names."""
        question_lower = question.lower()
        
        # Common player nicknames/short names
        player_patterns = {
            'c√¥ng ph∆∞·ª£ng': 'Nguy·ªÖn C√¥ng Ph∆∞·ª£ng',
            'quang h·∫£i': 'Nguy·ªÖn Quang H·∫£i',
            'vƒÉn to√†n': 'Nguy·ªÖn VƒÉn To√†n',
            'vƒÉn h·∫≠u': 'ƒêo√†n VƒÉn H·∫≠u',
            'ti·∫øn linh': 'Nguy·ªÖn Ti·∫øn Linh',
            'ƒë·ª©c chinh': 'H√† ƒê·ª©c Chinh',
            'h√πng d≈©ng': 'ƒê·ªó H√πng D≈©ng',
            'xu√¢n tr∆∞·ªùng': 'L∆∞∆°ng Xu√¢n Tr∆∞·ªùng',
            'tu·∫•n anh': 'Nguy·ªÖn Tu·∫•n Anh',
            'ho√†ng ƒë·ª©c': 'Nguy·ªÖn Ho√†ng ƒê·ª©c',
            'vƒÉn l√¢m': 'ƒê·∫∑ng VƒÉn L√¢m',
            'vƒÉn quy·∫øt': 'Nguy·ªÖn VƒÉn Quy·∫øt',
            'anh ƒë·ª©c': 'Nguy·ªÖn Anh ƒê·ª©c',
            'minh v∆∞∆°ng': 'Tr·∫ßn Minh V∆∞∆°ng',
            'hu·ª≥nh nh∆∞': 'C√π Th·ªã Hu·ª≥nh Nh∆∞',
        }
        
        for pattern, full_name in player_patterns.items():
            if pattern in question_lower:
                return full_name
        
        return None
    
    def _get_player_clubs(self, player: str) -> List[str]:
        """Get all clubs a player has played for."""
        result = self.kg.execute_cypher("""
            MATCH (p:Player {name: $name})-[:PLAYED_FOR]->(c:Club)
            RETURN c.name as club
        """, {"name": player})
        return [r['club'] for r in result] if result else []
        
    def _extract_entities(self, text: str) -> List[str]:
        """Extract entities from text."""
        entities = []
        text = re.sub(r'[?!.,;:]', ' ', text)
        words = text.split()
        
        for n in range(min(5, len(words)), 0, -1):
            for i in range(len(words) - n + 1):
                phrase = " ".join(words[i:i+n])
                matches = self.kg.search_entities(phrase, limit=1)
                if matches:
                    entities.append(matches[0].name)
                    
        return list(dict.fromkeys(entities))
    
    def _check_played_for(self, player: str, club: str) -> bool:
        """Check if player played for specific club.
        
        Uses strict matching to avoid false positives.
        """
        rels = self.kg.get_entity_relationships(player)
        club_lower = club.lower()
        
        # Extract key identifier from club name (usually location name)
        club_key_words = []
        location_keywords = ['h√† n·ªôi', 's√†i g√≤n', 'ƒë√† n·∫µng', 'h·∫£i ph√≤ng', 'thanh h√≥a', 
                           'ngh·ªá an', 'qu·∫£ng nam', 'qu·∫£ng ninh', 'ninh b√¨nh', 'b√¨nh d∆∞∆°ng',
                           'long an', 'c·∫ßn th∆°', 'vƒ©nh long', 'gia lai', 'hu·∫ø', 'kh√°nh h√≤a',
                           'b√¨nh ƒë·ªãnh', 't√¢y ninh', 'ƒë·ªìng nai', 'h·ªì ch√≠ minh', 'nam ƒë·ªãnh',
                           'th√°i b√¨nh', 'ƒë·ªìng th√°p', 'an giang', 'b√¨nh ph∆∞·ªõc', 'l√¢m ƒë·ªìng']
        
        for kw in location_keywords:
            if kw in club_lower:
                club_key_words.append(kw)
        
        for rel in rels:
            if rel.relation_type in ["PLAYED_FOR", "PLAYED_FOR_NATIONAL"]:
                target_lower = rel.target.name.lower()
                
                # Exact match
                if club_lower == target_lower:
                    return True
                
                # One fully contains the other (strict substring)
                if club_lower in target_lower or target_lower in club_lower:
                    # Additional check: key location must match
                    if club_key_words:
                        for kw in club_key_words:
                            if kw in target_lower:
                                return True
                    else:
                        return True
                
        return False
    
    def _check_born_in(self, player: str, place: str) -> bool:
        """Check if player was born in specific place."""
        rels = self.kg.get_entity_relationships(player)
        place_lower = place.lower()
        for rel in rels:
            if rel.relation_type in ["BORN_IN", "FROM_PROVINCE"]:
                target_lower = rel.target.name.lower()
                if place_lower in target_lower or target_lower in place_lower:
                    return True
        return False
    
    def _check_national_team(self, player: str) -> bool:
        """Check if player played for national team."""
        rels = self.kg.get_entity_relationships(player)
        for rel in rels:
            if rel.relation_type == "PLAYED_FOR_NATIONAL":
                return True
        return False
        
    def answer_yes_no(self, question: str) -> Tuple[str, float]:
        """Answer yes/no question with multi-hop reasoning."""
        entities = self._extract_entities(question)
        question_lower = question.lower()
        
        # Pattern 1: "X v√† Y t·ª´ng ch∆°i c√πng c√¢u l·∫°c b·ªô" (2-hop)
        if "c√πng c√¢u l·∫°c b·ªô" in question_lower or "ch∆°i c√πng" in question_lower:
            if len(entities) >= 2:
                same_club, club = self.kg.check_same_club(entities[0], entities[1])
                if same_club:
                    return "C√≥", 0.95
                same_club, club = self.kg.check_same_club(entities[1], entities[0])
                if same_club:
                    return "C√≥", 0.95
                return "Kh√¥ng", 0.85
        
        # Pattern 2: "X v√† Y l√† ƒë·ªìng ƒë·ªôi" (1-hop or 2-hop)
        if "ƒë·ªìng ƒë·ªôi" in question_lower:
            if len(entities) >= 2:
                is_teammate, info = self.kg.check_teammates(entities[0], entities[1])
                if is_teammate:
                    return "C√≥", 0.95
                is_teammate, info = self.kg.check_teammates(entities[1], entities[0])
                if is_teammate:
                    return "C√≥", 0.95
                return "Kh√¥ng", 0.85
        
        # Pattern 3: "X t·ª´ng ch∆°i cho CLB ·ªü t·ªânh c√≥ c·∫ßu th·ªß Y" (3-hop)
        if "·ªü t·ªânh c√≥ c·∫ßu th·ªß" in question_lower or "t·ªânh c√≥ c·∫ßu th·ªß" in question_lower:
            if len(entities) >= 2:
                played_in_province, club, province = self.kg.check_played_in_province_of_player(
                    entities[0], entities[1]
                )
                if played_in_province:
                    return "C√≥", 0.95
                return "Kh√¥ng", 0.85
        
        # Pattern 4: "X c√πng qu√™ v·ªõi Y" (2-hop via province)
        if "c√πng qu√™" in question_lower or "c√πng t·ªânh" in question_lower:
            if len(entities) >= 2:
                same_province, province = self.kg.check_same_province_via_club(entities[0], entities[1])
                if same_province:
                    return "C√≥", 0.95
                return "Kh√¥ng", 0.85
        
        # Pattern 5: "X t·ª´ng ch∆°i cho Y" (1-hop) - SPECIFIC CHECK
        if ("t·ª´ng ch∆°i cho" in question_lower or "ch∆°i cho" in question_lower) and len(entities) >= 2:
            player = entities[0]
            club = entities[1]
            if self._check_played_for(player, club):
                return "C√≥", 0.95
            # Try reverse (in case entity order is wrong)
            if self._check_played_for(club, player):
                return "C√≥", 0.95
            return "Kh√¥ng", 0.90
        
        # Pattern 6: "X sinh ra t·∫°i Y" (1-hop) - SPECIFIC CHECK
        if ("sinh ra" in question_lower or "sinh t·∫°i" in question_lower or "qu√™" in question_lower) and len(entities) >= 2:
            player = entities[0]
            place = entities[1]
            if self._check_born_in(player, place):
                return "C√≥", 0.95
            return "Kh√¥ng", 0.90
        
        # Pattern 7: "X c√≥ t·ª´ng kho√°c √°o ƒë·ªôi tuy·ªÉn qu·ªëc gia" (1-hop)
        if "ƒë·ªôi tuy·ªÉn qu·ªëc gia" in question_lower or "kho√°c √°o" in question_lower:
            if entities:
                player = entities[0]
                if self._check_national_team(player):
                    return "C√≥", 0.95
                return "Kh√¥ng", 0.90
        
        # Fallback: Only answer "C√≥" if we find EXACT matching relationship
        if entities and len(entities) >= 2:
            entity1 = entities[0]
            entity2 = entities[1]
            
            # Get relationships and check for direct connection with matching type
            rels = self.kg.get_entity_relationships(entity1)
            entity2_lower = entity2.lower()
            
            for rel in rels:
                target_lower = rel.target.name.lower()
                # Check if entity2 matches target
                if entity2_lower in target_lower or target_lower in entity2_lower:
                    return "C√≥", 0.9
            
            # Check reverse direction
            rels2 = self.kg.get_entity_relationships(entity2)
            entity1_lower = entity1.lower()
            
            for rel in rels2:
                target_lower = rel.target.name.lower()
                if entity1_lower in target_lower or target_lower in entity1_lower:
                    return "C√≥", 0.9
                    
            # No direct relationship found
            return "Kh√¥ng", 0.7
        
        # Single entity - check if question asks about specific property
        if entities:
            return "Kh√¥ng", 0.6
        
        return "Kh√¥ng", 0.6
        
    def answer_mcq(self, question: str, choices: List[str]) -> Tuple[str, float, str]:
        """Answer multiple choice question."""
        entities = self._extract_entities(question)
        
        if not entities:
            return choices[0] if choices else "", 0.25, "Kh√¥ng t√¨m th·∫•y th·ª±c th·ªÉ"
            
        entity = entities[0]
        rels = self.kg.get_entity_relationships(entity)
        
        # Score each choice
        best_choice = choices[0] if choices else ""
        best_score = 0.0
        explanation = ""
        
        for choice in choices:
            score = 0.0
            
            # Check if choice matches any relationship target
            for rel in rels:
                if choice.lower() in rel.target.name.lower() or rel.target.name.lower() in choice.lower():
                    score = 0.9
                    explanation = rel.to_text()
                    break
                    
            # Check partial match
            if score == 0:
                choice_words = set(choice.lower().split())
                for rel in rels:
                    target_words = set(rel.target.name.lower().split())
                    overlap = len(choice_words & target_words)
                    if overlap > 0:
                        score = 0.5 * (overlap / max(len(choice_words), len(target_words)))
                        
            if score > best_score:
                best_score = score
                best_choice = choice
                
        return best_choice, best_score, explanation
        
    def close(self):
        """Clean up."""
        if self.kg:
            self.kg.close()


class HybridChatbot:
    """
    Hybrid Chatbot combining:
    1. Graph Reasoning (SimpleChatbot) - For accurate answers from Knowledge Graph
    2. Small LLM (Qwen2-0.5B) - For natural language formatting
    
    This approach satisfies:
    - Requirement (1): Uses LLM ‚â§ 1B params
    - Requirement (2): Uses Knowledge Graph + GraphRAG
    - Requirement (3): Multi-hop reasoning via graph traversal
    
    The key insight: LLM is used to FORMAT the answer, not to FIND the answer.
    Graph reasoning finds the accurate answer, LLM makes it sound natural.
    """
    
    def __init__(self, model_name: str = None, device: str = None):
        self.model_name = model_name or MODEL_NAME
        self.device = device or DEVICE
        
        # LLM components
        self.model = None
        self.tokenizer = None
        self.generator = None
        
        # Graph reasoning component (SimpleChatbot)
        self.graph_reasoner = SimpleChatbot()
        
        self._initialized = False
        
    def initialize(self) -> bool:
        """Initialize all components."""
        logger.info("üöÄ Initializing Hybrid Chatbot...")
        
        # Initialize graph reasoner first (critical component)
        logger.info("üìä Initializing Graph Reasoner...")
        if not self.graph_reasoner.initialize():
            logger.error("Failed to initialize graph reasoner")
            return False
        logger.info("‚úÖ Graph Reasoner ready")
        
        # Initialize LLM (optional, for formatting)
        logger.info(f"ü§ñ Loading LLM for formatting: {self.model_name}")
        self._load_model()  # Don't fail if LLM doesn't load
        
        self._initialized = True
        logger.info("‚úÖ Hybrid Chatbot initialized!")
        return True
        
    def _load_model(self) -> bool:
        """Load the small language model for formatting."""
        try:
            if self.device == "cuda" and torch.cuda.is_available():
                dtype = torch.float16
                device_map = "auto"
            else:
                dtype = torch.float32
                device_map = None
                self.device = "cpu"
                
            logger.info(f"Loading model on {self.device}...")
            
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True
            )
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=dtype,
                device_map=device_map,
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            
            self.generator = pipeline(
                "text-generation",
                model=self.model,
                tokenizer=self.tokenizer,
                device_map=device_map,
                torch_dtype=dtype
            )
            
            logger.info(f"‚úÖ LLM loaded: {self.model_name}")
            return True
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to load LLM: {e}")
            logger.info("Will use graph reasoning without LLM formatting")
            return False
    
    def answer_yes_no(self, question: str) -> Tuple[str, float, str]:
        """
        Answer yes/no question using Graph Reasoning.
        LLM is NOT used for finding the answer - only graph traversal.
        
        Returns:
            Tuple of (answer, confidence, explanation)
        """
        # Use graph reasoning to get the accurate answer
        answer, confidence = self.graph_reasoner.answer_yes_no(question)
        
        # Generate explanation based on graph data
        entities = self.graph_reasoner._extract_entities(question)
        explanation = f"D·ª±a tr√™n ƒë·ªì th·ªã tri th·ª©c v·ªõi c√°c th·ª±c th·ªÉ: {', '.join(entities[:3]) if entities else 'kh√¥ng x√°c ƒë·ªãnh'}"
        
        return answer, confidence, explanation
    
    def answer_mcq(self, question: str, choices: List[str]) -> Tuple[str, float, str]:
        """
        Answer MCQ using Graph Reasoning.
        
        Returns:
            Tuple of (selected_choice, confidence, explanation)
        """
        return self.graph_reasoner.answer_mcq(question, choices)
    
    def chat(self, question: str) -> str:
        """
        Answer a question using Hybrid approach:
        1. Graph Reasoning finds the answer
        2. (Optional) LLM formats the response
        """
        # Step 1: Get answer from graph reasoning
        graph_answer = self.graph_reasoner.chat(question)
        
        # Step 2: If LLM is available, format the response
        if self.generator and len(graph_answer) > 10:
            try:
                formatted = self._format_with_llm(question, graph_answer)
                return formatted
            except Exception as e:
                logger.warning(f"LLM formatting failed: {e}")
                return graph_answer
        
        return graph_answer
    
    def _format_with_llm(self, question: str, graph_answer: str) -> str:
        """Use LLM to format the graph answer more naturally."""
        prompt = f"""<|im_start|>system
B·∫°n l√† tr·ª£ l√Ω AI v·ªÅ b√≥ng ƒë√° Vi·ªát Nam. ƒê·ªãnh d·∫°ng l·∫°i c√¢u tr·∫£ l·ªùi cho t·ª± nhi√™n h∆°n.
Gi·ªØ nguy√™n th√¥ng tin ch√≠nh x√°c, ch·ªâ l√†m cho c√¢u tr·∫£ l·ªùi t·ª± nhi√™n h∆°n.<|im_end|>
<|im_start|>user
C√¢u h·ªèi: {question}
Th√¥ng tin t·ª´ ƒë·ªì th·ªã: {graph_answer}
H√£y tr·∫£ l·ªùi ng·∫Øn g·ªçn, t·ª± nhi√™n:<|im_end|>
<|im_start|>assistant
"""
        
        try:
            outputs = self.generator(
                prompt,
                max_new_tokens=100,
                temperature=0.3,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                return_full_text=False
            )
            
            response = outputs[0]['generated_text'].strip()
            # Clean up
            response = re.sub(r'<\|.*?\|>', '', response)
            
            # If LLM response is too short or weird, return original
            if len(response) < 5 or response.lower() == graph_answer.lower():
                return graph_answer
                
            return response
            
        except Exception:
            return graph_answer
    
    @property
    def kg(self):
        """Access to knowledge graph."""
        return self.graph_reasoner.kg
    
    def close(self):
        """Clean up resources."""
        if self.graph_reasoner:
            self.graph_reasoner.close()
        if self.model:
            del self.model
        if torch.cuda.is_available():
            torch.cuda.empty_cache()


# Factory function
def create_chatbot(use_llm: bool = True, hybrid: bool = False) -> Any:
    """Create and initialize a chatbot instance.
    
    Args:
        use_llm: If True and hybrid=False, use GraphRAGChatbot (pure LLM)
        hybrid: If True, use HybridChatbot (Graph + LLM formatting)
    """
    if hybrid:
        chatbot = HybridChatbot()
    elif use_llm:
        chatbot = GraphRAGChatbot()
    else:
        chatbot = SimpleChatbot()
        
    chatbot.initialize()
    return chatbot
